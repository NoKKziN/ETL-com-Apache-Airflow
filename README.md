# ğŸ“Œ ETL com Apache Airflow â€” IntegraÃ§Ã£o API Cosmos â†’ PostgreSQL

Este repositÃ³rio contÃ©m meu **primeiro projeto utilizando Apache Airflow**, desenvolvido para orquestrar um fluxo ETL simples e modularizado.  
O objetivo principal foi consumir dados de uma API (CosmosPro), armazenÃ¡-los em uma tabela *staging* e, posteriormente, carregÃ¡-los em uma tabela final no PostgreSQL.

---

## ğŸš€ Objetivos do Projeto

- Criar um **pipeline ETL real** usando Airflow.  
- Consumir dados de uma API autenticada via conexÃ£o configurada no Airflow.  
- Salvar registros em uma tabela de *staging* para manter o processo mais seguro e escalÃ¡vel.  
- Efetuar a carga final em uma tabela limpa, seguindo boas prÃ¡ticas de ETL.  
- Estruturar as funÃ§Ãµes de forma modular, facilitando a leitura, manutenÃ§Ã£o e evoluÃ§Ã£o do pipeline.

---

## ğŸ§© Arquitetura Geral

O fluxo ETL foi dividido em duas etapas principais:

### **1ï¸âƒ£ ExtraÃ§Ã£o para Staging**

- Autentica na API CosmosPro usando uma conexÃ£o criada no Airflow (`api_cosmos_conn`).
- Realiza uma requisiÃ§Ã£o POST Ã  API.
- Cria (se nÃ£o existir) e limpa a tabela `staging_usuarios_api`.
- Insere os dados brutos retornados pela API na tabela de staging.

### **2ï¸âƒ£ Carga Final**

- Garante a existÃªncia da tabela `usuarios_api`.
- Limpa a tabela antes da nova carga.
- Copia os dados da tabela de staging para a tabela final.

---

## ğŸ—ï¸ Estrutura da DAG

A DAG (`dag_etl_cosmos_modularizado`) Ã© composta por dois **PythonOperators**:

```mermaid
flowchart LR
    A[extrair_para_staging] --> B[carregar_dados_finais]
```

## ğŸ”§ Tecnologias Utilizadas

- **Apache Airflow**
- **Python**
- **PostgreSQL**
- **Airflow Hooks**
  - `BaseHook` â€” autenticaÃ§Ã£o da API
  - `PostgresHook` â€” conexÃ£o com o banco de dados
- **Requests** â€” biblioteca para chamadas HTTP

---

## ğŸ“‚ OrganizaÃ§Ã£o das FunÃ§Ãµes

O cÃ³digo foi modularizado em:

- FunÃ§Ãµes de **conexÃ£o** (API e Postgres)
- FunÃ§Ã£o de **fechamento seguro** de conexÃ£o
- FunÃ§Ã£o de **extraÃ§Ã£o**
- FunÃ§Ã£o de **carga**
- DefiniÃ§Ã£o da **DAG**

Essa estrutura torna o fluxo mais claro, organizado e fÃ¡cil de manter.

---

## ğŸ“˜ O que aprendi neste projeto

Este foi meu primeiro contato direto com o Airflow. Durante o desenvolvimento, aprendi:

- Como criar **DAGs e operadores**
- Como usar **Hooks** para conectar com API e banco de dados
- Como estruturar um **ETL modular**
- Como trabalhar com tabelas de **staging**
- Como organizar um **pipeline escalÃ¡vel** de forma profissional
